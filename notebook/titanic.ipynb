{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction\n",
    "\n",
    "This is a short lesson on data cleaning and classification prediction, using Kaggle's \n",
    "introductory competition, [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Importing The Data\n",
    "We're going to be using Pandas to work with our data, so we'll simply import our two \n",
    "csv files directly into Pandas DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../input/train.csv' does not exist: b'../input/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-739-04e9bbba2ed1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/titanic/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/titanic/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/titanic/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/titanic/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/titanic/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../input/train.csv' does not exist: b'../input/train.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To simplify our data processing, we can combine these two data frames into a single\n",
    "data frame. Since the test data frame is missing the 'Survived' column, we'll fill \n",
    "this in with `np.nan`, so we can remember which rows are test data and which are \n",
    "training data later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_df['Survived'] = np.nan\n",
    "df = train_df.append(test_df, sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With that out of the way, let's start looking at the actual contents of our data and \n",
    "put it into a form that'll best suited for training our classifier.\n",
    "\n",
    "# Clean-up Time \n",
    "\n",
    "Now that we've loaded our data, we need to clean it up. Let's take a look at what we're \n",
    "dealing with first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If we want to look for linear or binary relationships between \n",
    "our data and the survived value of each passenger, we need to convert this human-readable \n",
    "data into something that's more easy for a machine to understand: numbers!\n",
    "\n",
    "Let's go through this column-by-column, explaining the process for each new case as \n",
    "we come across it.\n",
    "\n",
    "# Sex\n",
    "The 'Sex' column is a great place to start because of its simplicity. There are only \n",
    "two cases to worry about, 'male' and 'female', and there isn't any missing data. So all \n",
    "we need to do is map these strings to a binary case for a classifier algorithm to understand.\n",
    "\n",
    "Let's use the mapping `{0: 'female', 1: 'male'}`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'female': 0,\n",
    "    'male': 1\n",
    "}\n",
    "df['Sex'] = df['Sex'].apply(lambda x: mapping[x])\n",
    "df['Sex'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With just one step, we have our first binary column! Simple enough. Now onto the next one...\n",
    "\n",
    "# Age\n",
    "The 'Age' column is already in a nice numeric form, but let's take a look into how well \n",
    "it actually corresponds to whether a passenger survived in its current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.isna(df['Age']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Looks like we have some NA values! Considering that age is a nice continuous value, let's just assign\n",
    "the median age to each missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "med_age = np.median(df['Age'][~pd.isna(df['Age'])])\n",
    "df['Age'] = df['Age'].fillna(med_age)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we have only real values, let's look at the correspondence between age and survival:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df[['Survived', 'Age']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Quick refresher on correspondence: this value ranges between -1 and +1. \n",
    "\n",
    "A correspondence of 1 means two sets are strongly linearly \n",
    "correlated. Ideally, with a correspondence of 1, you could apply a simple formula of the form \n",
    "`y = mx + b` to map from one set to the other, with `m` being positive.\n",
    "\n",
    "A correspondence of -1 means the two sets are strongly linearly correlated as well, just ini the negative \n",
    "direction. You can still apply the mapping `y = mx + b`, but now you'd have a negative `m`.\n",
    "\n",
    "A correspondence of 0 means that the two sets you're looking at have no similarity. There exists no mapping \n",
    "from one set to the other.\n",
    "\n",
    "Back to our data! Seeing that the correspondence between age and survival is nearly zero, we need to \n",
    "see if there's anything we can do to actually compare the two columns! One popular method \n",
    "of identifying nonlinear correspondence is by binning, or making bins of ages, so we have a bin filled with \n",
    "our 0-10 year old passengers, our 11-20 year old passengers, and so on and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bin_space = 10\n",
    "bin_walls = range(0, 100, bin_space)\n",
    "bin_intervals = [pd.Interval(low, low + bin_space) for low in bin_walls]\n",
    "bins = pd.IntervalIndex(bin_intervals)\n",
    "\n",
    "age_bin_mapping = {}\n",
    "for i, b in enumerate(bins):\n",
    "    age_bin_mapping[b] = i\n",
    "\n",
    "age_bins = pd.cut(df['Age'], bins=bins)\n",
    "df['Age_bin'] = age_bins.apply(lambda x: age_bin_mapping[x]).astype(int)\n",
    "\n",
    "df[['PassengerId', 'Survived', 'Age_bin']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we have all of our passengers assigned to bins! Let's see what the correspondence is for this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df[['Survived', 'Age_bin']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As expected, the age bins linearly correspond with survival just as little as the raw age did.\n",
    "There might be a strong correspondence hiding here still, so let's look at the survival rate for each bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for bin_ind in age_bin_mapping.values():\n",
    "    df_bin = df[df['Age_bin'] == bin_ind]\n",
    "    if len(df_bin) == 0:\n",
    "        continue\n",
    "    print(\"Bin {} survival rate: {:.2g}\".format(\n",
    "        bin_ind, \n",
    "        len(df_bin[df_bin['Survived'] == 1]) / len(df_bin[~pd.isna(df_bin['Survived'])])\n",
    "    ))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Looking at this breakdown, there's definitely some sort of trend. Bin 0 had a much higher \n",
    "rate of survival than the others, while bins 6 and 7 had much lower rates of survival. In \n",
    "order to extract this in a more machine-readable way, we're going to expand this categorical \n",
    " column into a set of columns, one for each bin, with the value `1` if the row belongs to \n",
    " the bin and `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "age_bins = pd.get_dummies(df['Age_bin']).add_prefix('Age_bin_')\n",
    "df = df.merge(age_bins, how='left', left_index=True, right_index=True, validate='1:1')\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we have a DataFrame with 8 extra columns, one for each age bin that we had created earlier.\n",
    "Since we've coded the age column into this new form, we can drop the old columns so we \n",
    "don't try to train on different representations of the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Age', 'Age_bin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pclass (Ticket class)\n",
    "Now that we've finished cleaning up the age data, let's move on to `Pclass`. First, we should look at its\n",
    "correspondence to see if it's already good or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.338481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>-0.338481</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Survived    Pclass\n",
       "Survived  1.000000 -0.338481\n",
       "Pclass   -0.338481  1.000000"
      ]
     },
     "execution_count": 740,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Survived', 'Pclass']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A correspondence of -0.3 is actually fairly strong, compared to our previous examinations\n",
    "of the age bins. We can break this value down further to see if there are any individual \n",
    "ticket classes that show strong individual correspondences with the survival rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passenger class 1: 0.63\n",
      "Passenger class 2: 0.47\n",
      "Passenger class 3: 0.24\n"
     ]
    }
   ],
   "source": [
    "p_classes = sorted(df['Pclass'].unique())\n",
    "for p_class in p_classes:\n",
    "    df_class = df[df['Pclass'] == p_class]\n",
    "    survival = len(df_class[df_class['Survived'] == 1]) / len(df_class[~pd.isna(df_class['Survived'])])\n",
    "    print(\"Passenger class {}: {:.2g}\".format(p_class, survival))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With these survival rates, it looks like there is a pretty nice linear relationship in the \n",
    "data already, so we'll leave the Pclass column as-is.\n",
    "\n",
    "And just to be safe, we should check to make sure that there aren't any missing data\n",
    "from this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[pd.isna(df['Pclass'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "No missing values, good! Now we can move on to the next column to analyze.\n",
    "\n",
    "# SibSP (Siblings and spouses)\n",
    "\n",
    "Now we'll look at the column 'SibSp', the number of siblings and spouses each passenger had. As always, let's first take a look at the linear correspondence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['SibSp'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-743-037e3c7d8c1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Survived'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SibSp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/titanic/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2999\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3000\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3001\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3003\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/titanic/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1283\u001b[0m                 \u001b[0;31m# When setting, missing keys are not allowed, even with .loc:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"raise_missing\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_setter\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/titanic/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         self._validate_read_indexer(\n\u001b[0;32m-> 1092\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m         )\n\u001b[1;32m   1094\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/titanic/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"loc\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} not in index\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnot_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['SibSp'] not in index\""
     ]
    }
   ],
   "source": [
    "df[['Survived', 'SibSp']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there's essentially no correspondece here, so we need to dissect these values a bit to see if there's anything more to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sib_nums = sorted(df['SibSp'].unique())\n",
    "\n",
    "print(\"Survival rates:\")\n",
    "for s in sib_nums:\n",
    "    df_s = df[df['SibSp'] == s]\n",
    "    survival = len(df_s[df_s['Survived'] == 1]) / len(df_s)\n",
    "    print(\"{} siblings/spouses: {:.2g} \\t(population: {})\".format(s, survival, len(df_s[~pd.isna(df_s['Survived'])])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like there's a line that can be drawn around a 'SibSp' value of 4-5. Families with many siblings or spouses didn't seem to have any survivability, so that'll make for a strong feature in our classifier. Let's make a new binary column for 'LargeSibSp' to track this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largeSibSpCutOff = 4\n",
    "df['LargeSibSp'] = df['SibSp'] > largeSibSpCutOff\n",
    "\n",
    "df_l = df[df['LargeSibSp'] == True]\n",
    "df_s = df[df['LargeSibSp'] == False]\n",
    "print(\"Survival for large SibSp: {}\".format(len(df_l[df_l['Survived'] == 1]) / len(df_l[~pd.isna(df_l['Survived'])])))\n",
    "print(\"Survival for small SibSp: {}\".format(len(df_s[df_s['Survived'] == 1]) / len(df_s[~pd.isna(df_s['Survived'])])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before moving on to the next feature, let's check for, and clean up, any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[pd.isna(df['SibSp'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Parch (Parents and children)\n",
    "\n",
    "The number of parents and children seems like a very similar mesaurement to the number of siblings and spouses. We should keep that in mind and come back to that later. For now though, we'll look at this feature on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Survived', 'Parch']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No significant correspondence can be observed here, so let's run the same steps we did with 'SibSp':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parch_nums = sorted(df['Parch'].unique())\n",
    "\n",
    "print(\"Survival rates:\")\n",
    "for p in parch_nums:\n",
    "    df_p = df[df['Parch'] == p]\n",
    "    survival = len(df_p[df_p['Survived'] == 1]) / len(df_p)\n",
    "    print(\"{} parents/children: {:.2g} \\t(population: {})\".format(p, survival, len(df_p[~pd.isna(df_p['Survived'])])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like 'SibSp', 'Parch shows a strong line between small numbers of parents/children and large. This is a bit trickier though; since there is a non-zero survival rate at 5, we can't draw our line of survival as easily as we did for 'SibSp'. Let's try breaking this into three new columns: 'SmallParch', 'MedParch', and 'LargeParch'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SmallParch'] = df['Parch'] <= 3\n",
    "df['MedParch'] = df['Parch'].between(4, 5)  # Boundaries are included\n",
    "df['LargeParch'] = df['Parch'] >= 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s = df[df['SmallParch'] == True]\n",
    "df_m = df[df['MedParch'] == True]\n",
    "df_l = df[df['LargeParch'] == True]\n",
    "\n",
    "print(\"Survival for small Parch: {:.2g}\".format(len(df_s[df_s['Survived'] == 1]) / len(df_s[~pd.isna(df_s['Survived'])])))\n",
    "print(\"Survival for medium Parch: {:.2g}\".format(len(df_m[df_m['Survived'] == 1]) / len(df_m[~pd.isna(df_m['Survived'])])))\n",
    "print(\"Survival for large Parch: {:.2g}\".format(len(df_l[df_l['Survived'] == 1]) / len(df_l[~pd.isna(df_l['Survived'])])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, as usual, we should check for any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[pd.isna(df['Parch'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Now, remember that note we wanted to remember earlier about how 'Parch' and 'SibSp' sound very similar? Let's explore that now.\n",
    "\n",
    "# SibSp + Parch\n",
    "\n",
    "The columns 'SibSp' and 'Parch' showed very similar trends, so adding the two together might give even better data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Relatives'] = df['SibSp'] + df['Parch']\n",
    "\n",
    "nums = sorted(df['Relatives'].unique())\n",
    "\n",
    "print(\"Survival rates:\")\n",
    "for p in nums:\n",
    "    df_p = df[df['Relatives'] == p]\n",
    "    survival = len(df_p[df_p['Survived'] == 1]) / len(df_p[~pd.isna(df_p['Survived'])])\n",
    "    print(\"{} SibSp + Parch: {:.2g} \\t(population: {})\".format(p, survival, len(df_p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like, just as predicted, this data is essentially identical to what we found in each of the two individual columns above. Looking more closely, we can identify an almost-clean bell curve, centered a 'Relatives' size of around 2 or 3. The only data that throws this off is the increase in survivability from 'Relatives' values of 5 to 6. For now, let's ignore that as an anomaly and see if we can capture this Gaussian-like relationship around some ideal family size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_relatives = 2\n",
    "\n",
    "df['DistFromIdealRel'] = np.abs(df['Relatives'] - ideal_relatives)\n",
    "\n",
    "df[['Survived', 'DistFromIdealRel']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An absolute correspondence of 0.25 is much better than what we started with for 'SibSp' or 'Parch' before (around 0.07), so this seems like it'll be a decent feature. Now we can go back in and drop the original columns from the table so they don't add noise to our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['SibSp', 'Parch', 'Relatives'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fare\n",
    "\n",
    "So far so good! Next, time to examine the relationship between the fare, or ticket price, and survival rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Survived', 'Fare']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have one of the best correspondences we've seen! Let's keep this column as-is and move on for now.\n",
    "\n",
    "# Cabin\n",
    "\n",
    "Now it's time to explore whether a passenger's cabin location had anything to do with survivability. Before checking correspondences, I'd like to get any missing values out of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NA values: {}\".format(len(df[pd.isna(df['Cabin'])])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a fairly significant amount of missing cabin information, so I'd like to hold off on filling in this data until we know what the existing relationship looks like.\n",
    "\n",
    "Let's first see if the fact that we're missing cabin information can tell us anything about survival rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cabin = df[pd.isna(df['Cabin'])]\n",
    "yes_cabin = df[~pd.isna(df['Cabin'])]\n",
    "print(len(no_cabin[no_cabin['Survived'] == 1]) / len(no_cabin[~pd.isna(no_cabin['Survived'])]))\n",
    "print(len(yes_cabin[yes_cabin['Survived'] == 1]) / len(yes_cabin[~pd.isna(yes_cabin['Survived'])]))\n",
    "print(len(df[df['Survived'] == 1]) / len(df[~pd.isna(df['Survived'])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, it looks like having a cabin on record does affect the survival rate of the passengers (at least in our data set)! In that case, we're going to keep these \"no-cabin\" labels as their own unique property, while we break down the values for passengers who do have cabins.\n",
    "\n",
    "For cabin values, we'll start by binning them based on the cabin class (the letter preceding the cabin number). There are a few cabins that have the letter F before their actual cabin class, so let's separate the F-marked passengers for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "def get_cabin_class(cabin):\n",
    "    if pd.isna(cabin):\n",
    "        return np.nan\n",
    "    \n",
    "    title_search = re.search('([A-Za-z ]+)', cabin)\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    \n",
    "    return np.nan\n",
    "\n",
    "df['CabinClass'] = df[~pd.isna(df['Cabin'])]['Cabin'].apply(get_cabin_class)\n",
    "\n",
    "cabin_classes = sorted(df[~pd.isna(df['CabinClass'])]['CabinClass'].unique())\n",
    "print(cabin_classes)\n",
    "\n",
    "print(\"Survival rates:\")\n",
    "for c in cabin_classes:\n",
    "    df_c = df[df['CabinClass'] == c]\n",
    "    survival = len(df_c[df_c['Survived'] == 1]) / len(df_c[~pd.isna(df_c['Survived'])])\n",
    "    print(\"CabinClass {}:\\t{:.2g} \\t(population: {})\".format(c, survival, len(df_c)))\n",
    "    \n",
    "df = df.drop(columns=['CabinClass'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly enough, it looks like this doesn't really give us any more information than we already had, aside from a small percentage of outliers in 'F G' and 'T' cabin classes. Maybe we'll circle back around to this to see if there are any slight improvements that can be made from the slight discrepancies between each cabin class..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh well, it was worth a shot! Let's just stick with our original finding then, the difference in survival rate between those with a recorded cabin number, and those without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['HasCabin'] = ~pd.isna(df['Cabin'])\n",
    "df = df.drop(columns=['Cabin'])\n",
    "df[['Survived', 'HasCabin']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name\n",
    "\n",
    "This one will be tricky. Maybe there are correspondences between the survival rates of passengers with certain titles? Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(name):\n",
    "    if pd.isna(name):\n",
    "        return np.nan\n",
    "\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    \n",
    "    return np.nan\n",
    "\n",
    "df['Title'] = df['Name'].apply(get_title)\n",
    "titles = df['Title'].unique()\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few categories we could specify to split this into groups. \"Is a doctor\", or \"is military\", or \"is married\" (for women at least).\n",
    "\n",
    "Before any of those specifics, let's first just check the survival rate of each of these individually. Along with some extra clean-up combining titles that mean the same thing (and mapping unknown marital status titles to the same thing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_map = {\n",
    "    'Miss': 'Ms',\n",
    "    'Mlle': 'Ms',\n",
    "    'Sir':  'Master',\n",
    "    'Don':  'Master',\n",
    "    'Jonkheer': 'Master',\n",
    "    'Countess': 'Mme',\n",
    "    'Lady':     'Mme',\n",
    "    'Dona':     'Mme'\n",
    "}\n",
    "\n",
    "def map_title(title):\n",
    "    if pd.isna(title):\n",
    "        return np.nan\n",
    "    \n",
    "    if title in title_map.keys():\n",
    "        return title_map[title]\n",
    "    else:\n",
    "        return title\n",
    "    \n",
    "df['Title'] = df['Title'].apply(map_title)\n",
    "titles = df['Title'].unique()\n",
    "\n",
    "print(\"Survival rates:\")\n",
    "for t in titles:\n",
    "    df_t = df[df['Title'] == t]\n",
    "    survival = len(df_t[df_t['Survived'] == 1]) / max(1, len(df_t[~pd.isna(df_t['Survived'])]))\n",
    "    print(\"{}:\\t{:.2g} \\t(population: {})\".format(t, survival, len(df_t[~pd.isna(df_t['Survived'])])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the old saying \"the captain must go down with the ship\" holds in this case! Also, \"Masters\" fared much better than men with \"Mr\". The women with \"Mme\" also fared quite well, even though there are only 3 in this case. Reverends, on the other hand, all went down with the ship.\n",
    "\n",
    "For code simplicity, let's just go ahead and expand this column into binary columns like we did previously with age groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_bins = pd.get_dummies(df['Title']).add_prefix('Title_')\n",
    "df = df.merge(title_bins, how='left', left_index=True, right_index=True, validate='1:1')\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Name', 'Title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embarked\n",
    "\n",
    "Now, almost done with the basic data cleaning for our data set! Next, it's time to look at the 'Embarked' column. From the data sheet, this column represents a passenger's port of embarkation and is encoded with one of three letters: C (Cherbourg), Q (Queenstown), or S (Southampton). Let's first take a look at the survival rate of passengers from each port to see if there's a trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ports = df['Embarked'].unique()\n",
    "\n",
    "print(\"Survival rates:\")\n",
    "for p in ports:\n",
    "    if pd.isna(p):\n",
    "        df_p = df[pd.isna(df['Embarked'])]\n",
    "    else:\n",
    "        df_p = df[df['Embarked'] == p]\n",
    "    survival = len(df_p[df_p['Survived'] == 1]) / max(1, len(df_p[~pd.isna(df_p['Survived'])]))\n",
    "    print(\"{}:\\t{:.2g} \\t(population: {})\".format(p, survival, len(df_p[~pd.isna(df_p['Survived'])])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does look like there might be a trend here that could be worth extracting: passengers from port C show higher survival rates than those from the other two. Let's expand the 'Embarked' column into binary columns. \n",
    "\n",
    "Also, since the sample size of passengers with no port data is so small, we should go ahead and just add them to the most common port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df.groupby('Embarked').size()\n",
    "most_common_index = groups[groups == max(groups)].index[0]\n",
    "df['Embarked'] = df['Embarked'].fillna(most_common_index)\n",
    "\n",
    "print(most_common_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embarked_bins = pd.get_dummies(df['Embarked']).add_prefix('Embarked_')\n",
    "df = df.merge(embarked_bins, how='left', left_index=True, right_index=True, validate='1:1')\n",
    "\n",
    "df = df.drop(columns='Embarked')\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ticket\n",
    "Now, for the last field, the tickets. There seems to be a lot going on in this field, so we're going to have to do some string searching gymnastics to uncover any possble correlations to survival rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets = df['Ticket'].unique()\n",
    "print(len(tickets))\n",
    "print(tickets[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All tickets have a number, but only some have prefixes, and that prefix seems to have a hold of variation. Let's start cleaning this up to see how many different prefixes there really are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticket_prefix(ticket):\n",
    "    if pd.isna(ticket):\n",
    "        return np.nan\n",
    "\n",
    "    pref = re.sub('[0-9]+$', '', ticket).strip(' ')\n",
    "    return pref\n",
    "\n",
    "df['TicketPrefix'] = df['Ticket'].apply(ticket_prefix)\n",
    "prefixes = sorted(df['TicketPrefix'].unique())\n",
    "prefixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we've narrowed down the results just a bit. It looks like there are different formats for the same prefix in many cases, like `['A./5', 'A/5.', 'A/5']` or `['C.A.', 'CA']`. We should make an attempt to clean these up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_prefix(prefix):\n",
    "    prefix = prefix.lower()\n",
    "    prefix = prefix.replace(' ', '')\n",
    "    prefix = prefix.replace('.', '')\n",
    "    prefix = prefix.replace('/', '')\n",
    "    \n",
    "    return prefix\n",
    "\n",
    "df['TicketPrefix'] = df['TicketPrefix'].apply(simplify_prefix)\n",
    "prefixes = sorted(df['TicketPrefix'].unique())\n",
    "print(prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering these tickets were all commissioned manually - no computers back then - there could've been discrepencies in the way a ticket master wrote the details. Possibly `sop`, `sp`, and `sopp` were the same classification, as well as the pair `soton` and `ston`. And considering the prefixes starting with 'a', it really looks like `as` was a typo for `a5`. Without making too many assumptions, let's start with just these examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_prefix(prefix):\n",
    "    prefix = prefix.replace('sop', 'sp')\n",
    "    prefix = prefix.replace('sopp', 'sp')\n",
    "    prefix = prefix.replace('soton', 'ston')\n",
    "    prefix = prefix.replace('as', 'a5')\n",
    "    \n",
    "    return prefix\n",
    "\n",
    "df['TicketPrefix'] = df['TicketPrefix'].apply(simplify_prefix)\n",
    "prefixes = sorted(df['TicketPrefix'].unique())\n",
    "print(prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in prefixes:\n",
    "    df_p = df[df['TicketPrefix'] == p]\n",
    "    survival = len(df_p[df_p['Survived'] == 1]) / max(1, len(df_p[~pd.isna(df_p['Survived'])]))\n",
    "    print(\"{}:\\t{:.2g} \\t(population: {})\".format(p, survival, len(df_p[~pd.isna(df_p['Survived'])])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many of these ticket prefixes, there are too few passengers for us to really be able to draw any conclusion. Let's try merging some of the other prefixes that have similar survival rates and prefix structure, then expand these into new binary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prefix_bin(prefix):\n",
    "    if prefix == '':\n",
    "        return 'Empty'\n",
    "    if prefix[:1] == 'a':\n",
    "        return 'A'\n",
    "    if prefix[:1] == 'c':\n",
    "        return 'C'\n",
    "    if prefix[:1] == 'p':\n",
    "        return 'P'\n",
    "    if prefix[:1] == 'w':\n",
    "        return 'W'\n",
    "    \n",
    "    if prefix[:3] == 'fcc':\n",
    "        return 'FCC'\n",
    "    if prefix[:2] == 'sp':\n",
    "        return 'SP'\n",
    "    if prefix[:2] == 'scp':\n",
    "        return 'SCP'\n",
    "    if prefix[:4] == 'ston':\n",
    "        return 'STON'\n",
    "\n",
    "    return 'Other'\n",
    "\n",
    "df['TicketPrefix_Bin'] = df['TicketPrefix'].apply(prefix_bin)\n",
    "\n",
    "prefix_bins = pd.get_dummies(df['TicketPrefix_Bin']).add_prefix('TicketPrefix_')\n",
    "df = df.merge(prefix_bins, how='left', left_index=True, right_index=True, validate='1:1')\n",
    "\n",
    "df = df.drop(columns=['TicketPrefix_Bin'])\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now , at least, that's a wrap for data cleaning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Classification\n",
    "\n",
    "Now we can actually put our data to use and try to make a survival classifier out of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% Vizualize Data Relationships\n"
    }
   },
   "outputs": [],
   "source": [
    "corr = df.corr().abs()\n",
    "\n",
    "plt.matshow(df.corr().abs())\n",
    "cb = plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% Split data into X and y\n"
    }
   },
   "outputs": [],
   "source": [
    "num_cols = df.select_dtypes('number').columns\n",
    "df_numeric = df[num_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_numeric[~pd.isna(df_numeric['Survived'])]\n",
    "train_df = train_df.dropna()\n",
    "survived = train_df.pop('Survived')\n",
    "\n",
    "X = train_df.to_numpy()\n",
    "y = survived.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df_numeric[pd.isna(df_numeric['Survived'])]\n",
    "_ = test_df.pop('Survived')\n",
    "test_df = test_df.fillna(0)\n",
    "\n",
    "X_Evaluation = test_df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% Try SVM\n"
    }
   },
   "source": [
    "# Individual Classifier Performance\n",
    "\n",
    "We're going to start by looking at inidviudal classifiers, discussing their pros and cons for the type of data we're working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import ShuffleSplit, cross_validate, cross_val_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def best_fit_model(clf):\n",
    "    scoring = ['f1', 'roc_auc', 'accuracy']\n",
    "    scores = cross_validate(clf, X, y, scoring=scoring, cv=6, return_estimator=True)\n",
    "    comb = scores['test_f1'] * scores['test_roc_auc'] * scores['test_accuracy']\n",
    "\n",
    "    best_ind = np.argmax(comb)\n",
    "    print(\"Best model: \\n- F1: {:.3g} \\n- ROC AUC: {:.3g} \\n- Accuracy: {:.3g}\".format(scores['test_f1'][best_ind], \n",
    "                                                                  scores['test_roc_auc'][best_ind],\n",
    "                                                                  scores['test_accuracy'][best_ind]))\n",
    "    best_model = scores['estimator'][best_ind]\n",
    "    return best_model, scores\n",
    "\n",
    "estimators = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, criterion=\"entropy\")\n",
    "\n",
    "best_forest, svc_scores = best_fit_model(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP (Multi Layer Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "n = X.shape[1]\n",
    "clf = MLPClassifier(hidden_layer_sizes=(n, n, int(n/2), 6), random_state=0, max_iter=300)\n",
    "\n",
    "best_mlp, scores = best_fit_model(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=5, max_depth=10, criterion=\"entropy\"), \n",
    "                         n_estimators=5,\n",
    "                         random_state=0)\n",
    "\n",
    "best_adaboost, scores = best_fit_model(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Clasifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "estimators = [\n",
    "    ('Forest', best_forest),\n",
    "    ('MLP', best_mlp),\n",
    "    ('Adaboost', best_adaboost)\n",
    "]\n",
    "clf = VotingClassifier(estimators=estimators, voting='soft')\n",
    "best_eclf, scores = best_fit_model(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the Final Predicttions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% Generate the output for the actual test predictions\n"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = best_adaboost.predict(X_Evaluation)\n",
    "df_pred = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Survived': y_pred.astype(int)\n",
    "})\n",
    "\n",
    "df_pred.to_csv(\"../output/pred.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
